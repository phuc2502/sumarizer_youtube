"""
Summarization Module - H·ªó tr·ª£ 3 m·ª©c ƒë·ªô t√≥m t·∫Øt
"""

from groq import Groq
import streamlit as st

client = None

# ========================================
# C·∫§U H√åNH 3 M·ª®C ƒê·ªò T√ìM T·∫ÆT
# ========================================

SUMMARY_LEVELS = {
    "quick": {
        "name": "‚ö° T√≥m t·∫Øt nhanh",
        "description": "~200 t·ª´ ‚Ä¢ 5-7 ƒëi·ªÉm ch√≠nh ‚Ä¢ X·ª≠ l√Ω nhanh",
        "words": 200,
        "max_tokens": 600,
        "max_transcript": 12000,
        "prompt": """T√≥m t·∫Øt video YouTube sau trong kho·∫£ng 200 t·ª´ b·∫±ng {language}.
        
Y√äU C·∫¶U:
- Ch·ªâ tr√≠ch xu·∫•t 5-7 ƒëi·ªÉm ch√≠nh QUAN TR·ªåNG NH·∫§T
- M·ªói ƒëi·ªÉm ng·∫Øn g·ªçn, s√∫c t√≠ch (1-2 c√¢u)
- S·ª≠ d·ª•ng bullet points (‚Ä¢)
- Kh√¥ng l·∫∑p l·∫°i th√¥ng tin

FORMAT:
## üìå T√≥m t·∫Øt nhanh

‚Ä¢ [ƒêi·ªÉm 1]
‚Ä¢ [ƒêi·ªÉm 2]
...

**üí° K·∫øt lu·∫≠n:** [1 c√¢u t√≥m l·∫°i]
"""
    },
    
    "standard": {
        "name": "üìù T√≥m t·∫Øt chu·∫©n",
        "description": "~500 t·ª´ ‚Ä¢ 10-15 ƒëi·ªÉm ‚Ä¢ C√¢n b·∫±ng",
        "words": 500,
        "max_tokens": 1200,
        "max_transcript": 18000,
        "prompt": """T√≥m t·∫Øt chi ti·∫øt video YouTube sau trong kho·∫£ng 500 t·ª´ b·∫±ng {language}.
        
Y√äU C·∫¶U:
- Tr√≠ch xu·∫•t 10-15 ƒëi·ªÉm quan tr·ªçng
- Gi·∫£i th√≠ch ng·∫Øn g·ªçn t·ª´ng ƒëi·ªÉm
- Nh√≥m c√°c ƒëi·ªÉm theo ch·ªß ƒë·ªÅ n·∫øu c√≥ th·ªÉ
- S·ª≠ d·ª•ng bullet points v√† sub-bullets

FORMAT:
## üìù T√≥m t·∫Øt n·ªôi dung

### üéØ √ù ch√≠nh
‚Ä¢ [ƒêi·ªÉm ch√≠nh 1]
‚Ä¢ [ƒêi·ªÉm ch√≠nh 2]

### üìö Chi ti·∫øt
‚Ä¢ [Chi ti·∫øt 1]
  - [Gi·∫£i th√≠ch]
‚Ä¢ [Chi ti·∫øt 2]
  - [Gi·∫£i th√≠ch]

### üí° K·∫øt lu·∫≠n
[T√≥m t·∫Øt v√† nh·∫≠n x√©t]
"""
    },
    
    "detailed": {
        "name": "üìö T√≥m t·∫Øt chi ti·∫øt",
        "description": "~1500 t·ª´ ‚Ä¢ 20+ ƒëi·ªÉm ‚Ä¢ ƒê·∫ßy ƒë·ªß nh·∫•t",
        "words": 1500,
        "max_tokens": 3500,
        "max_transcript": 28000,
        "prompt": """T√≥m t·∫Øt R·∫§T CHI TI·∫æT video YouTube sau trong kho·∫£ng 1500 t·ª´ b·∫±ng {language}.
        
Y√äU C·∫¶U:
- Tr√≠ch xu·∫•t T·∫§T C·∫¢ th√¥ng tin quan tr·ªçng (20+ ƒëi·ªÉm)
- Gi·∫£i th√≠ch ƒë·∫ßy ƒë·ªß t·ª´ng ƒëi·ªÉm v·ªõi v√≠ d·ª• n·∫øu c√≥
- T·ªï ch·ª©c theo c·∫•u tr√∫c logic, ph√¢n chia sections r√µ r√†ng
- Bao g·ªìm c√°c chi ti·∫øt, s·ªë li·ªáu, v√≠ d·ª• ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p
- Th√™m nh·∫≠n x√©t v√† g·ª£i √Ω √°p d·ª•ng

FORMAT:
## üìö T√≥m t·∫Øt chi ti·∫øt

### üìå T·ªïng quan
[Gi·ªõi thi·ªáu ch·ªß ƒë·ªÅ, b·ªëi c·∫£nh c·ªßa video]

### üéØ N·ªôi dung ch√≠nh

#### 1. [Ph·∫ßn 1]
‚Ä¢ [ƒêi·ªÉm 1.1]: [Gi·∫£i th√≠ch chi ti·∫øt]
‚Ä¢ [ƒêi·ªÉm 1.2]: [Gi·∫£i th√≠ch chi ti·∫øt]

#### 2. [Ph·∫ßn 2]
‚Ä¢ [ƒêi·ªÉm 2.1]: [Gi·∫£i th√≠ch chi ti·∫øt]
‚Ä¢ [ƒêi·ªÉm 2.2]: [Gi·∫£i th√≠ch chi ti·∫øt]

#### 3. [Ph·∫ßn 3]
‚Ä¢ [ƒêi·ªÉm 3.1]: [Gi·∫£i th√≠ch chi ti·∫øt]

### üìä S·ªë li·ªáu/V√≠ d·ª• quan tr·ªçng
‚Ä¢ [S·ªë li·ªáu ho·∫∑c v√≠ d·ª• 1]
‚Ä¢ [S·ªë li·ªáu ho·∫∑c v√≠ d·ª• 2]

### üí° K·∫øt lu·∫≠n & √Åp d·ª•ng
[T√≥m t·∫Øt, nh·∫≠n x√©t, v√† c√°ch √°p d·ª•ng v√†o th·ª±c t·∫ø]

### üîó G·ª£i √Ω t√¨m hi·ªÉu th√™m
‚Ä¢ [Ch·ªß ƒë·ªÅ li√™n quan 1]
‚Ä¢ [Ch·ªß ƒë·ªÅ li√™n quan 2]
"""
    }
}


def truncate_transcript(transcript_text, max_length):
    """C·∫Øt transcript n·∫øu qu√° d√†i ƒë·ªÉ tr√°nh v∆∞·ª£t token limit."""
    if len(transcript_text) > max_length:
        truncated = transcript_text[:max_length]
        # T√¨m v·ªã tr√≠ k·∫øt th√∫c c√¢u g·∫ßn nh·∫•t
        last_period = truncated.rfind('.')
        if last_period > max_length * 0.8:
            truncated = truncated[:last_period + 1]
        return truncated + "\n\n[Transcript ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn do qu√° d√†i]"
    return transcript_text


def generate_summary_with_level(client, transcript_text, language, level="standard"):
    """
    T·∫°o b·∫£n t√≥m t·∫Øt v·ªõi m·ª©c ƒë·ªô chi ti·∫øt ƒë∆∞·ª£c ch·ªçn.
    
    Args:
        client: Groq client
        transcript_text: N·ªôi dung transcript
        language: Ng√¥n ng·ªØ output
        level: M·ª©c ƒë·ªô chi ti·∫øt ("quick", "standard", "detailed")
    
    Returns:
        str: B·∫£n t√≥m t·∫Øt
    """
    config = SUMMARY_LEVELS.get(level, SUMMARY_LEVELS["standard"])
    
    # Format prompt v·ªõi ng√¥n ng·ªØ
    formatted_prompt = config["prompt"].format(language=language)
    
    # C·∫Øt transcript theo gi·ªõi h·∫°n c·ªßa level
    truncated_transcript = truncate_transcript(transcript_text, config["max_transcript"])
    
    try:
        completion = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {
                    "role": "system",
                    "content": f"B·∫°n l√† chuy√™n gia t√≥m t·∫Øt n·ªôi dung video. Lu√¥n tr·∫£ l·ªùi b·∫±ng {language} v·ªõi format Markdown r√µ r√†ng."
                },
                {
                    "role": "user",
                    "content": formatted_prompt + "\n\n--- TRANSCRIPT ---\n" + truncated_transcript
                }
            ],
            temperature=0.7,
            max_tokens=config["max_tokens"],
            top_p=1,
            stream=True,
        )
        
        summary = ""
        for chunk in completion:
            if chunk.choices[0].delta.content:
                summary += chunk.choices[0].delta.content
        
        return summary
        
    except Exception as e:
        error_msg = str(e)
        if "rate_limit" in error_msg.lower():
            st.error("‚ö†Ô∏è ƒê√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n API. Vui l√≤ng ƒë·ª£i v√†i ph√∫t v√† th·ª≠ l·∫°i.")
        elif "invalid_api_key" in error_msg.lower():
            st.error("‚ùå API Key kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra l·∫°i.")
        elif "model" in error_msg.lower():
            st.warning("‚ö†Ô∏è Model ch√≠nh kh√¥ng kh·∫£ d·ª•ng. ƒêang th·ª≠ model d·ª± ph√≤ng...")
            return generate_with_fallback(client, formatted_prompt, truncated_transcript, config["max_tokens"])
        else:
            st.error(f"‚ùå L·ªói API: {error_msg}")
        return None


def generate_with_fallback(client, prompt, transcript, max_tokens):
    """S·ª≠ d·ª•ng model d·ª± ph√≤ng n·∫øu model ch√≠nh kh√¥ng kh·∫£ d·ª•ng."""
    fallback_models = ["llama-3.1-70b-versatile", "llama-3.1-8b-instant", "mixtral-8x7b-32768"]
    
    for model in fallback_models:
        try:
            completion = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt + "\n\nTranscript:\n" + transcript}],
                temperature=0.7,
                max_tokens=max_tokens,
                stream=True,
            )
            
            summary = ""
            for chunk in completion:
                if chunk.choices[0].delta.content:
                    summary += chunk.choices[0].delta.content
            
            st.success(f"‚úÖ ƒê√£ s·ª≠ d·ª•ng model d·ª± ph√≤ng: {model}")
            return summary
            
        except Exception:
            continue
    
    st.error("‚ùå T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau.")
    return None


# ========================================
# LEGACY SUPPORT - Gi·ªØ t∆∞∆°ng th√≠ch ng∆∞·ª£c
# ========================================

prompt_template = """Summarize the given YouTube video transcript in bullet points, focusing only on the most important information. The summary should be clear, concise, and within 250 words. Please summarize it in {language}."""

MAX_TRANSCRIPT_LENGTH = 15000

def generate_llama3_content(client, transcript_text, prompt, language):
    """Legacy function - S·ª≠ d·ª•ng generate_summary_with_level v·ªõi level standard."""
    return generate_summary_with_level(client, transcript_text, language, "standard")


@st.cache_data(show_spinner=True)
def get_summary(_client, transcript_text, language, video_id, level="standard"):
    """Generate and cache the summary based on the transcript and level."""
    cache_key = f"summary_{video_id}_{language}_{level}"
    
    if cache_key in st.session_state:
        return st.session_state[cache_key]
    
    summary = generate_summary_with_level(_client, transcript_text, language, level)
    
    st.session_state[cache_key] = summary
    
    return summary


def get_level_info(level):
    """L·∫•y th√¥ng tin v·ªÅ m·ªôt level c·ª• th·ªÉ."""
    return SUMMARY_LEVELS.get(level, SUMMARY_LEVELS["standard"])


def get_all_levels():
    """L·∫•y t·∫•t c·∫£ c√°c levels."""
    return SUMMARY_LEVELS
